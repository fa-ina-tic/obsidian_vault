### seq2seq의 단점
1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하다보니, 정보 손실이 발생
2. RNN 의 Vanishing Gradient

위의 단점을 해결하기 위해 고안된 방법이 **Attention**

### 기본 컨셉
Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다. 이를 참고할 때, 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 참고한다.

