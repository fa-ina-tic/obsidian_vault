기존 지식을 토대로 추론하는 거 
llm 과정을 생각해보면(transformer)

trained model / inference system  <---> ui(query)

학습은 오래 걸려도 되는데, 추론(사용자의 쿼리에 맞는 답안)을 뽑아내는 건 쥰내 빨라야겠죠?
